# Multimodal Biometric Authentication Using Lip Motion and Spoken Passphrases  
### BIOVID Challenge 2025 â€“ Dual-Factor Audioâ€“Visual Authentication  

**Authors:**  
- **Venu Siddapura Govindaraju** â€“ University of Naples Federico II, DIETI  
- **Stefano Marrone** â€“ University of Naples Federico II, DIETI  
- **Carlo Sansone** â€“ University of Naples Federico II, DIETI  

This repository contains the official implementation of our method submitted to the **BIOVID Challenge 2025**, titled:

> **â€œMultimodal Biometric Authentication Using Lip Motion and Spoken Passphrases.â€**  
> Accepted in *ICIAP 2025 Workshop â€“ BIOVID Challenge (LNCS Volume)*

Our system performs **open-set biometric authentication** using synchronized audioâ€“visual MP4 videos. Each sample includes:

- **Lip-motion RGB frames**  
- **Spoken passphrase audio**

We use a **dual-stream architecture**:  
3D-ResNet18 + BiGRU for visual encoding, and ECAPA-TDNN for audio encoding.  
A **Gated Multimodal Unit (GMU)** fuses both streams into a 256-D embedding used for classification and identity verification.

---

## ðŸ”¥ Key Contributions

- **Dual-stream audioâ€“visual architecture**
- **GMU-based adaptive fusion**
- **Hybrid Triplet + BCE loss**
- **Open-set decision using cosine similarity + threshold**
- **Full pipeline: preprocessing â†’ training â†’ inference â†’ submission**

---

## ðŸ§  System Architecture

The overall architecture of the proposed dual-stream audioâ€“visual authentication system is shown below:

![Architecture](architecture.jpg)


## Dataset
The BIOVID dataset is restricted and cannot be shared publicly.

To access it:

- **Register for the BIOVID Challenge 2025

- **Submit your method description

- **Receive download approval from the organizers

---
## ðŸ“‚ Project Structure

```text
Biovid-Challenge2025/
â”œâ”€â”€ data/                     # EXCLUDED â€“ BIOVID dataset not included
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ biovid_dataset.py     # Data loader + preprocessing
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ audio_encoder.py      # ECAPA-TDNN
â”‚   â”œâ”€â”€ visual_encoder.py     # 3D-ResNet18 + BiGRU
â”‚   â”œâ”€â”€ gmu_fusion.py         # Gated Multimodal Unit
â”‚   â”œâ”€â”€ fusion_head.py
â”‚   â””â”€â”€ output_head.py
â”œâ”€â”€ samplers/
â”‚   â””â”€â”€ triplet_sampler.py    # Semi-hard triplet mining
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â””â”€â”€ preprocess.py     # Frame extraction & audio extraction
â”‚   â”œâ”€â”€ inference/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ updated_pipeline.ipynb
â”‚   â””â”€â”€ 02_model_visual.ipynb
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ fold0_best_model.pt
â”‚   â”œâ”€â”€ fold1_best_model.pt
â”‚   â”œâ”€â”€ fold2_best_model.pt
â”‚   â””â”€â”€ gmu_fusion/
â”‚       â””â”€â”€ fold0_best_model.pt
â”œâ”€â”€ submission/
â”‚   â””â”€â”€ submission.json
â”œâ”€â”€ train_crossval.py
â”œâ”€â”€ test_inference_vote.py
â”œâ”€â”€ evaluate_eer.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

