# Multimodal Biometric Authentication Using Lip Motion and Spoken Passphrases  
### BIOVID Challenge 2025 â€“ Dual-Factor Lip-Based Authentication  
**Author:** Venu Siddapura Govindaraju  
**Affiliation:** University of Naples Federico II, DIETI

This repository contains the official implementation of our system submitted to the **BIOVID Challenge 2025**, titled:

**â€œMultimodal Biometric Authentication Using Lip Motion and Spoken Passphrases.â€**

The method performs **open-set user authentication** using synchronized audioâ€“visual MP4 videos. Each sample contains:

- **Lip-motion frames** (visual modality)  
- **Speech audio** (audio modality)  

The system extracts complementary biometric cues from both modalities using a *dual-stream encoder architecture* and fuses them using a **Gated Multimodal Unit (GMU)** to generate a discriminative 256-D joint embedding for both **classification** and **identity verification**.

---

# ğŸ”¥ Key Contributions

- Dual-stream biometric system combining **3D-ResNet-18 + BiGRU** (visual) and **ECAPA-TDNN** (audio).  
- **Gated Multimodal Unit (GMU)** for adaptive audioâ€“visual fusion.  
- Composite training loss using **Triplet Loss (semi-hard mining)** + **Binary Cross Entropy**.  
- **Open-set decision mechanism** using cosine similarity + threshold rejection.  
- Fully reproducible pipeline with preprocessing, training, inference, and submission generation.

---

# ğŸ§  System Architecture

Below is the full architecture corresponding exactly to the design in the paper.

ğŸ“Œ Place your architecture image inside the repo as:  
`architecture.jpg`

Then reference it in README (as shown below):

![Architecture](architecture.jpg)

---

# ğŸ“‚ Project Structure

```text
Biovid-Challenge2025/
â”œâ”€â”€ data/                     # EXCLUDED â€“ confidential BIOVID dataset
â”‚
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ biovid_dataset.py     # Video/audio reader + preprocessing
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ audio_encoder.py      # ECAPA-TDNN backbone
â”‚   â”œâ”€â”€ visual_encoder.py     # 3D-ResNet18 + BiGRU
â”‚   â”œâ”€â”€ gmu_fusion.py         # Gated Multimodal Unit
â”‚   â”œâ”€â”€ fusion_head.py
â”‚   â””â”€â”€ output_head.py
â”‚
â”œâ”€â”€ samplers/
â”‚   â””â”€â”€ triplet_sampler.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â””â”€â”€ preprocess.py     # Frame extraction, audio extraction
â”‚   â”œâ”€â”€ inference/
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ updated_pipeline.ipynb
â”‚   â””â”€â”€ 02_model_visual.ipynb
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ fold0_best_model.pt
â”‚   â”œâ”€â”€ fold1_best_model.pt
â”‚   â”œâ”€â”€ fold2_best_model.pt
â”‚   â””â”€â”€ gmu_fusion/
â”‚       â””â”€â”€ fold0_best_model.pt
â”‚
â”œâ”€â”€ submission/
â”‚   â””â”€â”€ submission.json
â”‚
â”œâ”€â”€ train_crossval.py
â”œâ”€â”€ test_inference_vote.py
â”œâ”€â”€ evaluate_eer.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
